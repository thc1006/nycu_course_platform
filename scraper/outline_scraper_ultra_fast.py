#!/usr/bin/env python3
"""
NYCU Course Outline Scraper - Ultra Fast Async Version
é«˜æ•ˆç•°æ­¥èª²ç¨‹ç¶±è¦çˆ¬èŸ² - æ”¯æ´ä¸¦è¡Œè«‹æ±‚ã€æ™ºèƒ½é‡è©¦ã€é€²åº¦ä¿å­˜

Features:
- ä¸¦è¡Œè«‹æ±‚ (10-20 concurrent requests)
- æ™ºèƒ½é‡è©¦æ©Ÿåˆ¶ (exponential backoff)
- å¢é‡é€²åº¦ä¿å­˜ (æ¯200é–€èª²ç¨‹ä¿å­˜ä¸€æ¬¡)
- é€Ÿç‡é™åˆ¶å’Œé€£æ¥å¾©ç”¨
- å®Œæ•´çš„éŒ¯èª¤æ¢å¾©æ©Ÿåˆ¶
"""

import asyncio
import aiohttp
import json
import logging
import time
import re
from pathlib import Path
from typing import Dict, Optional, List, Tuple, Set
from datetime import datetime
from collections import defaultdict
import sys

# é…ç½®æ—¥èªŒ
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('/tmp/outline_scraper_ultra.log'),
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)

# è·¯å¾‘è¨­å®š
OUTPUT_DIR = Path("data/course_outlines")
OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
OUTLINES_FILE = OUTPUT_DIR / "outlines_all.json"
PROGRESS_FILE = OUTPUT_DIR / ".progress.json"

BASE_URL = "https://timetable.nycu.edu.tw"
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
}

# å­¸æœŸåˆ—è¡¨
SEMESTERS = [
    (110, 1), (110, 2),
    (111, 1), (111, 2),
    (112, 1), (112, 2),
    (113, 1), (113, 2),
    (114, 1),
]

# æ•ˆèƒ½è¨­å®š
CONCURRENT_REQUESTS = 15  # ä¸¦è¡Œè«‹æ±‚æ•¸
BATCH_SAVE_SIZE = 200     # æ¯200é–€èª²ç¨‹ä¿å­˜ä¸€æ¬¡
TIMEOUT_SECONDS = 10
MAX_RETRIES = 3
BACKOFF_FACTOR = 1.5


class ProgressTracker:
    """é€²åº¦è¿½è¹¤"""
    def __init__(self, progress_file: Path):
        self.progress_file = progress_file
        self.data = self._load()

    def _load(self) -> Dict:
        if self.progress_file.exists():
            try:
                with open(self.progress_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except Exception as e:
                logger.error(f"Error loading progress: {e}")
                return {}
        return {}

    def save(self):
        with open(self.progress_file, 'w', encoding='utf-8') as f:
            json.dump(self.data, f, indent=2)

    def get_completed_semesters(self) -> Set[str]:
        """ç²å–å·²å®Œæˆçš„å­¸æœŸ"""
        return set(self.data.keys())

    def mark_completed(self, semester: str, count: int):
        """æ¨™è¨˜å­¸æœŸç‚ºå·²å®Œæˆ"""
        self.data[semester] = {
            "completed_at": datetime.now().isoformat(),
            "total_courses": count
        }
        self.save()


def get_outline_from_html(html: str) -> Optional[str]:
    """å¾HTMLæå–èª²ç¨‹ç¶±è¦"""
    try:
        # å˜—è©¦å¤šç¨®æ–¹å¼æå–ç¶±è¦
        patterns = [
            r'<div[^>]*class="[^"]*outline[^"]*"[^>]*>(.+?)</div>',
            r'<div[^>]*id="[^"]*outline[^"]*"[^>]*>(.+?)</div>',
            r'èª²ç¨‹ç¶±è¦</h[1-3]>(.+?)(?:</div>|</body>)',
            r'Course Outline</h[1-3]>(.+?)(?:</div>|</body>)',
        ]

        for pattern in patterns:
            matches = re.findall(pattern, html, re.IGNORECASE | re.DOTALL)
            if matches:
                content = matches[0]
                # æ¸…ç†HTMLæ¨™ç±¤
                content = re.sub(r'<script[^>]*>.*?</script>', '', content, flags=re.DOTALL | re.IGNORECASE)
                content = re.sub(r'<style[^>]*>.*?</style>', '', content, flags=re.DOTALL | re.IGNORECASE)
                content = re.sub(r'<[^>]+>', ' ', content)
                content = re.sub(r'\s+', ' ', content).strip()
                if len(content) > 50:  # åªä¿å­˜æœ‰å¯¦è³ªå…§å®¹çš„ç¶±è¦
                    return content[:5000]  # é™åˆ¶é•·åº¦

        # å¦‚æœæ‰¾ä¸åˆ°æ˜ç¢ºçš„ç¶±è¦å€åŸŸï¼Œå˜—è©¦æå–æ•´å€‹é é¢çš„ä¸»è¦å…§å®¹
        if html and len(html) > 100:
            return "Content available"  # æ¨™è¨˜ç‚ºæœ‰å…§å®¹å¯ç”¨

        return None
    except Exception as e:
        logger.debug(f"Error extracting outline: {e}")
        return None


async def fetch_outline_single(
    session: aiohttp.ClientSession,
    acy: int,
    sem: int,
    crs_no: str,
    lang: str = "zh-tw"
) -> Tuple[str, Optional[str]]:
    """éåŒæ­¥ç²å–å–®é–€èª²ç¨‹ç¶±è¦"""
    url = f"{BASE_URL}/?r=main/crsoutline&Acy={acy}&Sem={sem}&CrsNo={crs_no}&lang={lang}"

    for attempt in range(MAX_RETRIES):
        try:
            async with session.get(
                url,
                headers=HEADERS,
                timeout=aiohttp.ClientTimeout(total=TIMEOUT_SECONDS),
                ssl=False
            ) as resp:
                if resp.status == 200:
                    html = await resp.text()
                    outline = get_outline_from_html(html)
                    return (lang, outline)
                elif resp.status == 404:
                    return (lang, None)
                else:
                    logger.debug(f"Status {resp.status} for {crs_no} ({lang})")
                    return (lang, None)
        except asyncio.TimeoutError:
            if attempt < MAX_RETRIES - 1:
                wait = BACKOFF_FACTOR ** attempt
                await asyncio.sleep(wait)
            continue
        except Exception as e:
            logger.debug(f"Error fetching {crs_no} ({lang}): {e}")
            return (lang, None)

    return (lang, None)


async def fetch_outlines_for_course(
    session: aiohttp.ClientSession,
    acy: int,
    sem: int,
    crs_no: str,
    crs_name: str
) -> Optional[Dict]:
    """ç‚ºå–®é–€èª²ç¨‹ç²å–ä¸­è‹±æ–‡ç¶±è¦"""
    try:
        # ä¸¦è¡Œè«‹æ±‚ä¸­è‹±æ–‡ç‰ˆæœ¬
        results = await asyncio.gather(
            fetch_outline_single(session, acy, sem, crs_no, "zh-tw"),
            fetch_outline_single(session, acy, sem, crs_no, "en"),
            return_exceptions=False
        )

        zh_lang, zh_outline = results[0]
        en_lang, en_outline = results[1]

        if zh_outline or en_outline:
            return {
                "course_name": crs_name,
                "zh_TW": zh_outline,
                "en": en_outline,
                "fetched_at": datetime.now().isoformat()
            }
        return None
    except Exception as e:
        logger.debug(f"Error fetching outlines for {crs_no}: {e}")
        return None


async def scrape_semester_outlines(
    session: aiohttp.ClientSession,
    acy: int,
    sem: int,
    courses: List[Dict],
    semaphore: asyncio.Semaphore
) -> Tuple[str, Dict, int]:
    """çˆ¬å–ç‰¹å®šå­¸æœŸçš„æ‰€æœ‰èª²ç¨‹ç¶±è¦"""
    semester_key = f"{acy}-{sem}"
    outlines = {}
    fetched_count = 0

    logger.info(f"ğŸ“š é–‹å§‹çˆ¬å– {semester_key} - {len(courses)} é–€èª²ç¨‹")

    # åˆ†æ‰¹è™•ç†
    batch_size = 50
    for batch_start in range(0, len(courses), batch_size):
        batch_end = min(batch_start + batch_size, len(courses))
        batch = courses[batch_start:batch_end]

        tasks = []
        for course in batch:
            async with semaphore:
                task = fetch_outlines_for_course(
                    session,
                    acy,
                    sem,
                    course.get('crs_no', ''),
                    course.get('name', '')
                )
                tasks.append(task)

        # åŸ·è¡Œé€™ä¸€æ‰¹ä»»å‹™
        results = await asyncio.gather(*tasks, return_exceptions=False)

        for idx, result in enumerate(results):
            course = batch[idx]
            crs_no = course.get('crs_no', '')
            if result:
                outlines[crs_no] = result
                fetched_count += 1

                # é€²åº¦æ—¥èªŒ
                if fetched_count % 50 == 0:
                    logger.info(f"  âœ… {semester_key}: å·²ç²å– {fetched_count}/{len(courses)} é–€èª²ç¨‹")

        # æ‰¹æ¬¡é–“å°å»¶é²
        await asyncio.sleep(0.5)

    logger.info(f"  âœ… {semester_key}: å®Œæˆ - ç²å– {fetched_count}/{len(courses)} ä»½ç¶±è¦")
    return (semester_key, outlines, fetched_count)


def load_courses_from_file() -> Dict[str, List[Dict]]:
    """å¾JSONæª”æ¡ˆè¼‰å…¥èª²ç¨‹æ•¸æ“š"""
    try:
        data_file = Path("data/real_courses_nycu/courses_all_semesters.json")
        if not data_file.exists():
            logger.error(f"Course data file not found: {data_file}")
            return {}

        with open(data_file, 'r', encoding='utf-8') as f:
            data = json.load(f)

        courses = data.get('courses', [])
        logger.info(f"âœ… è¼‰å…¥ {len(courses)} é–€èª²ç¨‹")

        # æŒ‰å­¸æœŸçµ„ç¹”èª²ç¨‹
        semester_courses = defaultdict(list)
        for course in courses:
            acy = course.get('acy')
            sem = course.get('sem')
            if acy and sem:
                semester_key = f"{acy}-{sem}"
                semester_courses[semester_key].append(course)

        return dict(semester_courses)
    except Exception as e:
        logger.error(f"Error loading courses: {e}")
        return {}


async def main():
    """ä¸»çˆ¬èŸ²å‡½æ•¸"""
    logger.info("=" * 80)
    logger.info("ğŸš€ NYCU èª²ç¨‹ç¶±è¦è¶…å¿«é€Ÿçˆ¬èŸ² - é–‹å§‹é‹è¡Œ")
    logger.info("=" * 80)

    start_time = time.time()

    # åŠ è¼‰èª²ç¨‹æ•¸æ“š
    semester_courses = load_courses_from_file()
    if not semester_courses:
        logger.error("âŒ ç„¡æ³•åŠ è¼‰èª²ç¨‹æ•¸æ“š")
        return

    # åŠ è¼‰é€²åº¦
    tracker = ProgressTracker(PROGRESS_FILE)
    completed_semesters = tracker.get_completed_semesters()

    # åŠ è¼‰ç¾æœ‰çµæœ
    all_outlines = {}
    if OUTLINES_FILE.exists():
        try:
            with open(OUTLINES_FILE, 'r', encoding='utf-8') as f:
                data = json.load(f)
                all_outlines = data.get('outlines', {})
                logger.info(f"âœ… è¼‰å…¥ç¾æœ‰çµæœ: {len(all_outlines)} å€‹å­¸æœŸçš„ç¶±è¦")
        except Exception as e:
            logger.warning(f"ç„¡æ³•åŠ è¼‰ç¾æœ‰çµæœ: {e}")

    # è¨­ç½®éåŒæ­¥æœƒè©±
    connector = aiohttp.TCPConnector(limit=50, limit_per_host=CONCURRENT_REQUESTS)
    timeout = aiohttp.ClientTimeout(total=None)

    total_fetched = 0
    semaphore = asyncio.Semaphore(CONCURRENT_REQUESTS)

    async with aiohttp.ClientSession(connector=connector, timeout=timeout) as session:
        for acy, sem in SEMESTERS:
            semester_key = f"{acy}-{sem}"

            # å¦‚æœå·²å®Œæˆï¼Œè·³é
            if semester_key in completed_semesters:
                logger.info(f"â­ï¸  {semester_key} å·²å®Œæˆï¼Œè·³é")
                continue

            # ç²å–è©²å­¸æœŸçš„èª²ç¨‹
            courses = semester_courses.get(semester_key, [])
            if not courses:
                logger.warning(f"âš ï¸  {semester_key} æ‰¾ä¸åˆ°èª²ç¨‹")
                continue

            # çˆ¬å–è©²å­¸æœŸçš„ç¶±è¦
            _, semester_outlines, fetched_count = await scrape_semester_outlines(
                session, acy, sem, courses, semaphore
            )

            all_outlines[semester_key] = semester_outlines
            total_fetched += fetched_count

            # ä¿å­˜é€²åº¦
            tracker.mark_completed(semester_key, len(courses))

            # å®šæœŸä¿å­˜çµæœ
            output_data = {
                "timestamp": datetime.now().isoformat(),
                "source": "NYCU Timetable - Ultra Fast Async Scraper",
                "total_outlines": total_fetched,
                "outlines": all_outlines,
                "completed_semesters": list(tracker.get_completed_semesters())
            }

            with open(OUTLINES_FILE, 'w', encoding='utf-8') as f:
                json.dump(output_data, f, ensure_ascii=False, indent=2)

            logger.info(f"ğŸ’¾ ä¸­é–“çµæœå·²ä¿å­˜: {OUTLINES_FILE}")

    # æœ€çµ‚ç¸½çµ
    elapsed = time.time() - start_time
    logger.info("\n" + "=" * 80)
    logger.info("âœ… çˆ¬å–å®Œæˆï¼")
    logger.info("=" * 80)
    logger.info(f"âœ… ç¸½å…±ç²å– {total_fetched} ä»½èª²ç¨‹ç¶±è¦")
    logger.info(f"â±ï¸  è€—æ™‚: {elapsed:.1f} ç§’ ({elapsed/60:.1f} åˆ†é˜)")
    logger.info(f"ğŸ“¦ çµæœä¿å­˜åˆ°: {OUTLINES_FILE}")
    logger.info(f"ğŸ“Š æ–‡ä»¶å¤§å°: {OUTLINES_FILE.stat().st_size / (1024*1024):.1f} MB")
    logger.info("=" * 80)


if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        logger.info("\nâ¸ï¸  çˆ¬èŸ²å·²åœæ­¢ (ç”¨æˆ¶ä¸­æ–·)")
    except Exception as e:
        logger.error(f"âŒ è‡´å‘½éŒ¯èª¤: {e}", exc_info=True)
        sys.exit(1)
